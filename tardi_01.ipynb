{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "precise-poster",
   "metadata": {},
   "source": [
    "# Reconstruction of X-Ray tomograms (Hamburg)\n",
    "\n",
    "### start with defining the dataset and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inside-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "from maximus48 import var\n",
    "from maximus48.tomo_proc3 import init_Npad, init_names_custom, F,rotscan\n",
    "from maximus48 import SSIM_131119 as SSIM_sf \n",
    "from maximus48.SSIM_131119 import SSIM_const\n",
    "from maximus48 import multiCTF2 as multiCTF\n",
    "\n",
    "\n",
    "from maximus48.tomo_proc3 import rotaxis_rough\n",
    "\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "# from scipy.ndimage import rotate\n",
    "# from maximus48.tomo_proc3 import rotate,rotrough_compute\n",
    "\n",
    "from pybdv import make_bdv \n",
    "\n",
    "from dask import delayed\n",
    "import dask.array as da\n",
    "from dask.array.image import imread\n",
    "from dask.distributed import Client, progress\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "import tomopy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "satellite-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "#           initial parameters for phase retrieval with CTF\n",
    "# =============================================================================\n",
    "N_steps = 10                                                                   # Number of projections per degree\n",
    "N_start = 1                                                                    # index of the first file\n",
    "N_finish = 3600                                                                # index of the last file\n",
    "\n",
    "pixel = 0.1625 * 1e-6                                                          # pixel size \n",
    "distance = np.array((6.1, 6.5, 7.1, 8), dtype = 'float32') * 1e-2              # distances of your measurements \n",
    "energy = 18                                                                    # photon energy in keV\n",
    "beta_delta = 0.15\n",
    "zero_compensation = 0.05\n",
    "ROI = (0,100,2048,2048) \n",
    "                                                      # ROI of the image to be read (x,y,x1,y1 at the image - inverse to numpy!)\n",
    "cp_count = 48 \n",
    "chunksz = 30\n",
    "\n",
    "                                                               # number of cores for multiprocessing\n",
    "inclination = -0.23                                                            # angle to compansate the tilt of the rotaxis\n",
    "\n",
    "#data_name = 'ew21_5'\n",
    "data_name = 'Platy-12601'\n",
    "folder_base = '/scratch/schorb/HH_platy'\n",
    "folder = os.path.join(folder_base,'raw/')\n",
    "folder_temp = os.path.join(folder_base,'tmp/')\n",
    "folder_result = os.path.join(folder_base,'rec/')\n",
    "distances = (1,2,3,4)\n",
    "N_distances  = 4  \n",
    "\n",
    "\n",
    "#calculate parameters for phase-retrieval\n",
    "wavelength = var.wavelen(energy)\n",
    "fresnelN = pixel**2/(wavelength*distance)\n",
    "\n",
    "\n",
    "#create save folder if it doesn't exist\n",
    "\n",
    "if not os.path.exists(folder_temp):\n",
    "    os.makedirs(folder_temp)\n",
    "\n",
    "\n",
    "if not os.path.exists(folder_result):\n",
    "    os.makedirs(folder_result)\n",
    "\n",
    "\n",
    "# Variable structure:\n",
    "            # ['N_files',\n",
    "            #  'N_start',\n",
    "            #  'Npad',\n",
    "            #  'ROI',\n",
    "            #  'ROI_ff',            \n",
    "            #  'flats',\n",
    "            #  'im_shape',\n",
    "            #  'images',\n",
    "            #  'init_paths']\n",
    "            \n",
    "            \n",
    "Npad = init_Npad(ROI, compression = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Support functions\n",
    "# =============================\n",
    "\n",
    "def init_paths(data_name, path, distance_indexes):\n",
    "    \"\"\"Generate paths images & flatfields\"\"\"\n",
    "\n",
    "    #set data_names\n",
    "    data_names, ff_names = init_names_custom(data_name = data_name,\n",
    "                                             distance_indexes = distance_indexes)\n",
    "    \n",
    "    #find images\n",
    "    imlist = var.im_folder(path)\n",
    "    \n",
    "    # create a filter for unnecessary images\n",
    "    tail = '_00000.tiff'\n",
    "    \n",
    "    if len(data_names[0])!=len(data_names[-1]):\n",
    "        print(\"\"\"\n",
    "        WARNING! Different distances in your dataset \n",
    "        have different naming lengths. \n",
    "        File names can't be arranged. \n",
    "        Try to reduce the number of distances (<10) or modify the script.\n",
    "        \"\"\")\n",
    "        sys.exit()\n",
    "    else:\n",
    "        data_lencheck = len(data_names[0]+tail)\n",
    "        ff_lencheck = len(ff_names[0]+tail)\n",
    "    \n",
    "\n",
    "    \n",
    "    #set proper paths\n",
    "    N_distances = len(distance_indexes) \n",
    "    images = np.zeros(N_distances, 'object') \n",
    "    flats = np.zeros(N_distances, 'object')\n",
    "            \n",
    "    for i in np.arange(len(images)):\n",
    "        \n",
    "        #sort image paths\n",
    "        images[i] = [path+im for im in imlist \n",
    "                     if (im.startswith(data_names[i])) \n",
    "                     and not (im.startswith('.'))\n",
    "                     and (len(im) == data_lencheck)]\n",
    "        \n",
    "        flats[i] = [path+im for im in imlist \n",
    "                    if im.startswith(ff_names[i])\n",
    "                    and (len(im)==ff_lencheck)]\n",
    "\n",
    "    return images,flats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================\n",
    "\n",
    "\n",
    "## holographic reconstruction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_flat(j,images=[], ROI_ff=[], ROI=[],flats=[],ff_file='',ffcon_file='',distances=(), N_start=0, Npad=0):\n",
    "    \"\"\"\n",
    "    j: int\n",
    "        an index of the file that should be processed \n",
    "    Please note, j always starts from zero\n",
    "    To open correct file, images array uses images[i][j + N_start-1]\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "        \n",
    "    ff_con = np.load(ffcon_file,allow_pickle=True)\n",
    "    \n",
    "    ff = np.load(ff_file,allow_pickle=True)\n",
    "   \n",
    "    #read image and do ff-retrieval \n",
    "    \n",
    "    # =============================\n",
    "\n",
    "\n",
    "    \n",
    "    ## FLAT field correction\n",
    "    \n",
    "    \n",
    "    filt = []\n",
    "    \n",
    "    for i in np.arange(len(images)):\n",
    "        im = imread(images[i][j])[ROI[1]:ROI[3], ROI[0]:ROI[2]]\n",
    "     \n",
    "        maxcorridx=np.argmax(SSIM_sf.SSIM(SSIM_const(im[ROI_ff[1]:ROI_ff[3], ROI_ff[0]:ROI_ff[2]]),ff_con[i]).ssim())        \n",
    "        filt.append(im/ff[i][maxcorridx])\n",
    "\n",
    "        \n",
    "        \n",
    "    im_gau0 = var.filt_gauss_laplace(filt[0][ROI_ff[1]:ROI_ff[3], ROI_ff[0]:ROI_ff[2]],\n",
    "                                    sigma = 5)\n",
    "    thisshift = []\n",
    "    \n",
    "    for i in range(len(filt)):\n",
    "        im_gau1 = var.filt_gauss_laplace(filt[i][ROI_ff[1]:ROI_ff[3], ROI_ff[0]:ROI_ff[2]],\n",
    "                                    sigma = 5)\n",
    "        thisshift.append(var.shift_distance(im_gau0, im_gau1, 10))\n",
    "    \n",
    "    \n",
    "    filt0 = multiCTF.shift_imageset(filt, thisshift)\n",
    "\n",
    "    filt0 = np.pad(filt0, ((0,0),(Npad, Npad),(Npad, Npad)), 'edge')               # padding with border values\n",
    "    filt0 = multiCTF.multi_distance_CTF(filt0, beta_delta, \n",
    "                                          fresnelN, zero_compensation)\n",
    "    filt0 = filt0[Npad:(filt0.shape[0]-Npad),:]\n",
    "\n",
    "    imsave(os.path.join(folder_temp,''.join(os.path.basename(images[0][j]).partition('_'+str(distances[0]))[0:3:2])),filt0)\n",
    "    # pda = da.from_array(filt0)\n",
    "    # da.to_zarr(pda,'/scratch/schorb/HH_platy/Platy-12601_'+str(j)+'.zarr')\n",
    "\n",
    "    return 'done processing image '+str(j)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-currency",
   "metadata": {},
   "source": [
    "## Holographic reconstruction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "\n",
    "\n",
    "# RUN  SCRIPT\n",
    "\n",
    "# =============================\n",
    "\n",
    "\n",
    "images, flats = init_paths(data_name, folder, distances)\n",
    "\n",
    "im_shape = (ROI[3]-ROI[1], ROI[2]-ROI[0])\n",
    "\n",
    "shape_ff = (N_distances, len(flats[0]), im_shape[0], im_shape[1])\n",
    "ff_shared = F(shape = shape_ff, dtype = 'd')\n",
    "\n",
    "\n",
    "#read ff-files to memory\n",
    "\n",
    "ff = np.zeros(shape_ff)\n",
    "\n",
    "for i in range(N_distances):\n",
    "    for j,fname in enumerate(flats[i]):\n",
    "        ff[i][j]=imread(fname)[ROI[1]:ROI[3], ROI[0]:ROI[2]]\n",
    "        \n",
    "\n",
    "\n",
    "#calculate ff-related constants\n",
    "ROI_ff = (ff.shape[3]//4, ff.shape[2]//4,3 * ff.shape[3]//4, 3 * ff.shape[2]//4)    # make ROI for further flatfield and shift corrections, same logic as for normal ROI\n",
    "\n",
    "\n",
    "ff_con = np.zeros(N_distances, 'object')                                                # array of classes to store flatfield-related constants\n",
    "for i in np.arange(N_distances):    \n",
    "    ff_con[i] = SSIM_const(ff[i][:,ROI_ff[1]:ROI_ff[3], \n",
    "                                    ROI_ff[0]:ROI_ff[2]].transpose(1,2,0))\n",
    "\n",
    "ffcon_file = folder_temp+'ffcon.npy'\n",
    "np.save(ffcon_file,ff_con)\n",
    "\n",
    "\n",
    "\n",
    "ff_file = folder_temp+'ff.npy'\n",
    "np.save(ff_file,ff)\n",
    "\n",
    "\n",
    "#read_flat(j,images=images, ROI_ff=ROI_ff, ROI=ROI,flats=flats,distances=distances,ffcon_file=ffcon_file,ff_file=ff_file, N_start=N_start, Npad=Npad)\n",
    "\n",
    "#%%\n",
    "# s1=client.map....\n",
    "\n",
    "status = 'p'\n",
    "\n",
    "while status != 'done':\n",
    "    for st in s1:\n",
    "        \n",
    "        if st.status in ['error']:\n",
    "            print('retrying '+st.key)\n",
    "            \n",
    "            st.retry()\n",
    "            status = 'p'\n",
    "            time.sleep(1)\n",
    "        elif st.status in ['finished']:\n",
    "            status = 'done'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-diamond",
   "metadata": {},
   "source": [
    "## Stripe removal and storage of intermediate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "imfiles = sorted(glob.glob(folder_temp+'*.tiff'))\n",
    "\n",
    "im = imread(imfiles[0])\n",
    "pshape = (3600,im.shape[1],im.shape[2])\n",
    "\n",
    "proj = np.zeros(pshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,imf in enumerate(imfiles):proj[idx,:]=imread(imf)\n",
    "\n",
    "print('stripe removal\\n\\n================================\\n\\n')\n",
    "\n",
    "proj = tomopy.prep.stripe.remove_stripe_fw(proj,level=3, wname=u'db25', sigma=2, pad = False,ncore=cp_count,nchunk=chunksz)\n",
    "\n",
    "\n",
    "stripe_file = folder_temp+'/stripe.npy'\n",
    "np.save(stripe_file,proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-bookmark",
   "metadata": {},
   "source": [
    "## Read data back in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-moses",
   "metadata": {},
   "source": [
    "### Make it parallel..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dangerous-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = np.lib.format.open_memmap(folder_temp+'/stripe.npy',mode='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "pregnant-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = np.load(folder_temp+'/stripe.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "searching-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "projd = da.from_array(proj,chunks = [1,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "marine-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "\n",
    "def mmap_load_chunk(filename, shape, dtype, offset, sl):\n",
    "    '''\n",
    "    Memory map the given file with overall shape and dtype and return a slice\n",
    "    specified by :code:`sl`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    filename : str\n",
    "    shape : tuple\n",
    "        Total shape of the data in the file\n",
    "    dtype:\n",
    "        NumPy dtype of the data in the file\n",
    "    offset : int\n",
    "        Skip :code:`offset` bytes from the beginning of the file.\n",
    "    sl:\n",
    "        Object that can be used for indexing or slicing a NumPy array to\n",
    "        extract a chunk\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    numpy.memmap or numpy.ndarray\n",
    "        View into memory map created by indexing with :code:`sl`,\n",
    "        or NumPy ndarray in case no view can be created using :code:`sl`.\n",
    "    '''\n",
    "    data = np.memmap(filename, mode='r', shape=shape, dtype=dtype, offset=offset)\n",
    "    return data[sl]\n",
    "\n",
    "\n",
    "def mmap_dask_array(filename, shape, dtype, offset=0, blocksize=5):\n",
    "    '''\n",
    "    Create a Dask array from raw binary data in :code:`filename`\n",
    "    by memory mapping.\n",
    "\n",
    "    This method is particularly effective if the file is already\n",
    "    in the file system cache and if arbitrary smaller subsets are\n",
    "    to be extracted from the Dask array without optimizing its\n",
    "    chunking scheme.\n",
    "\n",
    "    It may perform poorly on Windows if the file is not in the file\n",
    "    system cache. On Linux it performs well under most circumstances.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    filename : str\n",
    "    shape : tuple\n",
    "        Total shape of the data in the file\n",
    "    dtype:\n",
    "        NumPy dtype of the data in the file\n",
    "    offset : int, optional\n",
    "        Skip :code:`offset` bytes from the beginning of the file.\n",
    "    blocksize : int, optional\n",
    "        Chunk size for the outermost axis. The other axes remain unchunked.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    dask.array.Array\n",
    "        Dask array matching :code:`shape` and :code:`dtype`, backed by\n",
    "        memory-mapped chunks.\n",
    "    '''\n",
    "    load = dask.delayed(mmap_load_chunk)\n",
    "    chunks = []\n",
    "    for index in range(0, shape[0], blocksize):\n",
    "        # Truncate the last chunk if necessary\n",
    "        chunk_size = min(blocksize, shape[0] - index)\n",
    "        chunk = dask.array.from_delayed(\n",
    "            load(\n",
    "                filename,\n",
    "                shape=shape,\n",
    "                dtype=dtype,\n",
    "                offset=offset,\n",
    "                sl=slice(index, index + chunk_size)\n",
    "            ),\n",
    "            shape=(chunk_size, ) + shape[1:],\n",
    "            dtype=dtype\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    return da.concatenate(chunks, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "expensive-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "projd = mmap_dask_array(\n",
    "    filename=folder_temp+'/stripe.npy',\n",
    "    shape=proj.shape,\n",
    "    dtype=proj.dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "projd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "known-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] ='1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "nuclear-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "client=Client(n_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "civic-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_image.ndinterp import rotate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "subjective-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = np.arange(0,projd.shape[0],100)\n",
    "slices = np.append(slices,projd.shape[0]-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "plain-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = []\n",
    "\n",
    "for idx,chunk in enumerate(slices[0:-1]):\n",
    "    p1=rotate(da.from_array(proj[chunk:slices[idx+1]]),inclination,axes=(1,2),reshape=True)\n",
    "    ra.append(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = client.compute(p1.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "sweet-aging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=[]\n",
    "\n",
    "for chunk in ra:\n",
    "    out.append(da.from_array(client.compute(chunk.compute()),chunks = [1,-1,-1]))\n",
    "    \n",
    "outarr = da.stack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "peaceful-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.to_zarr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "constant-cosmetic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Future: finalize</b> <font color=\"gray\">status: </font><font color=\"black\">finished</font>, <font color=\"gray\">type: </font>numpy.ndarray, <font color=\"gray\">key: </font>finalize-88a9011114aba553d9de036f3f0be7f5"
      ],
      "text/plain": [
       "<Future: finished, type: numpy.ndarray, key: finalize-88a9011114aba553d9de036f3f0be7f5>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_image.ndinterp import rotate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(p2[4,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-latex",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
